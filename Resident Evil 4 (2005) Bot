import os
import torch as T
import torch.nn as nn
import torch.optim as optim
import numpy as np
from pynput.keyboard import Controller, Key
import os
import cv2
from typing import TypeVar, Tuple , Dict
import time
import pyautogui
import pygetwindow as gw
import time

class MainNetwork(nn.Module):

    def __init__(self, model_path , checkpoint_path , n_actions , lr , input_dims):
        super().__init__()

        self.chekpoint_dir = checkpoint_path
        self.file = os.path.join(self.chekpoint_dir , model_path)

        self.conv1 = nn.Conv2d(input_dims[0] , 64 , 8 , 2 , 2)
        self.conv2 = nn.Conv2d(64 , 64 , 8 , 2 , 2)
        self.conv3 = nn.Conv2d(64 , 64 , 8 , 2 , 2)

        self.activation = nn.PReLU()
        self.optim = optim.Adam(self.parameters()  , lr = lr)
        self.loss = nn.MSELoss()
        self.device = T.device("cuda" if T.cuda.is_available() else "cpu")
        
        comp = self.comp_in_dims(input_dims)

        self.fc1 = nn.Linear(comp , 1024)
        self.fc2 = nn.Linear(1024 , 512)
        self.A = nn.Linear(512 , n_actions)
        self.V = nn.Linear(512 , 1)

    def forward(self , state):
        
        print(state)

        X = self.activation(self.conv1(state))
        X2 = self.activation(self.conv2(X))
        X3 = self.activation(self.conv3(X2))

        flatten = X3.view(X2.size()[0] , -1)

        A = self.fc1(flatten)
        V = self.fc2(A)

        return V , A
    
    def comp_in_dims(self, input_dims):

        dummy = T.zeros(1 , *input_dims)
        dims = self.conv1(dummy)
        dims = self.conv2(dims)
        dims = self.conv3(dims)

        return int(np.prod(dims.shape[1:]))
    
    def saving_model(self):
        print("... saving in progress ...")
        T.save(self.state_dict() , self.chekpoint_dir)

    def loading_mode_state_dict(self):
        print("... loading in progress ... ")
        self.load_state_dict(T.load(self.chekpoint_dir))

class ReplayBuffer():

    def __init__(self , max_buffer_size , lr , in_dims , n_actions):
        self.max = max_buffer_size
        self.mem_cntr = 0

        self.state_memory = np.zeros((self.max, *in_dims) , dtype = np.float32)
        self.update_state_memory =np.zeros((self.max, *in_dims) , dtype = np.float32)

        self.action_memory = np.zeros((self.max,) ,dtype= np.int32)
        self.reward_memory = np.zeros((self.max,) , dtype = np.float32)
        self.terminal_memory = np.zeros((self.max,) , dtype = bool)

    def sample_batches(self , state , update , n_actions , rewards , done):

        X = self.mem_cntr % self.max 

        self.state_memory[X] = state
        self.update_state_memory[X] = update
        self.action_memory[X] = n_actions
        self.reward_memory[X] = rewards
        self.terminal_memory[X] = done

        self.mem_cntr += 1

    def batch_memory(self , batch_size):

        max_ = max(self.mem_cntr , self.max )
        rnd = np.random.choice(max_ , batch_size , replace= False)

        state = self.state_memory[rnd]
        update =self.update_state_memory[rnd]
        n_actions = self.action_memory[rnd]
        rewards = self.reward_memory[rnd]
        terminal = self.terminal_memory[rnd]

        return state , update , n_actions , rewards , terminal
    
class Agent():

    def __init__(self , lr , input_dims , n_actions ,  max_buffer_size , batch_size ,
                 replace = 1000 , eps = 1.0 , eps_dec = 1e-3 , eps_min = 1e-2 , gamma = 0.99,
                 model_path = None , chechpoint_path = None):
        self.lr = lr
        self.input_dims = input_dims
        self.n_actions= n_actions
        self.max_buffer_size = max_buffer_size
        self.eps = eps
        self.eps_dec = eps_dec
        self.eps_min = eps_min
        self.gamma = gamma 
        self.model_path = model_path
        self.chechpoint_path = chechpoint_path 
        self.batch_size = batch_size
        self.action_space = [i for i in range(self.n_actions)]
        self.replace_target_network = replace
        self.learn_rate_step_size = 0
        
        self.memory = ReplayBuffer(
            in_dims= self.input_dims,
            max_buffer_size= self.max_buffer_size,
            lr = self.lr,
            n_actions= self.n_actions)
        
        self.Main_Network = MainNetwork(
            model_path= self.model_path,
            checkpoint_path=self.chechpoint_path,
            n_actions= self.n_actions,
            lr=self.lr,
            input_dims=self.input_dims)
        
        self.Target_Network = MainNetwork(
            model_path= self.model_path,
            checkpoint_path=self.chechpoint_path,
            n_actions= self.n_actions,
            lr=self.lr,
            input_dims=self.input_dims)
        
    def choose_actions(self, observations):
        if np.random.random() > self.eps:
            np_arr = np.array([observations], dtype=np.float32)  
            arr_to_tensor = T.tensor(np_arr).to(self.Main_Network.device)

            with T.no_grad(): 
                V, advantages = self.Main_Network.forward(arr_to_tensor)

            Q_values = V + (advantages - advantages.mean(dim=1, keepdim=True))
            actions = T.argmax(Q_values, dim=1).item() 
        else:
            actions = np.random.choice(self.action_space)

        return actions
    
    def dec_eps(self):

        self.eps = self.eps - self.eps_dec if self.eps > self.eps_min else self.eps_min

    def replace_target_network_fun(self):

        if self.Target_Network is not None and \
        self.replace_target_network % self.learn_rate_step_size == 0:
            self.Target_Network.load_state_dict(self.Main_Network.state_dict())

    def sample_obs_batches(self, state , update , n_actions , rewards , done):

        self.memory.sample_batches(state , update , n_actions , rewards , done)
    
    def sample_Main_batches(self):

        state , update , n_actions , rewards , done = self.memory.batch_memory(self.batch_size)

        states = T.tensor(state).to(self.Main_Network.device)
        updates = T.tensor(update).to(self.Main_Network.device)
        n_action = T.tensor(n_actions).to(self.Main_Network.device)
        reward = T.tensor(rewards).to(self.Main_Network.device)
        terminal = T.tensor(done).to(self.Main_Network.device)

        return states , updates , n_action , reward , terminal
    
    def learn(self):

        if self.memory.mem_cntr < self.batch_size:
            return

        self.replace_target_network_fun()

        state_batch, next_state_batch, action_batch, reward_batch, done_batch = self.sample_obs_batches()

        states = T.tensor(state_batch).to(self.Main_Network.device)
        next_states = T.tensor(next_state_batch).to(self.Main_Network.device)
        actions = T.tensor(action_batch, dtype=T.long).to(self.Main_Network.device)  
        rewards = T.tensor(reward_batch).to(self.Main_Network.device)
        dones = T.tensor(done_batch, dtype=T.bool).to(self.Main_Network.device)

        V_s, A_s = self.Main_Network.forward(states)
        Q_pred = V_s + (A_s - A_s.mean(dim=1, keepdim=True))

        Q_pred = Q_pred.gather(1, actions.unsqueeze(-1)).squeeze(-1)

        V_s_next, A_s_next = self.Target_Network.forward(next_states)
        Q_next = V_s_next + (A_s_next - A_s_next.mean(dim=1, keepdim=True))

        Q_next_max = Q_next.max(dim=1)[0]
        Q_next_max[dones] = 0.0
        Q_target = rewards + self.gamma * Q_next_max

        loss = self.Main_Network.loss(Q_target, Q_pred)
        self.Main_Network.zero_grad()
        loss.backward()
        self.Main_Network.optim.step()
        self.dec_eps()

class Actions():
    WALK_FORWARD = 0
    WALK_BACKWARD = 1
    AIM = 2
    SHOOT = 3
    RELOAD = 4
    TURN_AROUND_FAST = 5

class ResidentEvilEnv:

    ObsType = TypeVar("ObsType")
    ActType = TypeVar("ActType")

    def __init__(self):
        self.actions = {
            Actions.WALK_FORWARD: self.walking_forward,
            Actions.WALK_BACKWARD: self.walking_backward,
            Actions.AIM: self.aim,
            Actions.SHOOT: self.shoot,
            Actions.RELOAD : self.reload,
            Actions.TURN_AROUND_FAST : self.turn_around_fast}
        
        self.health = 100
        self.enemies_defeated = 0
        self.keyboard = Controller()

    def step(self, action: ActType) -> Tuple[ObsType, float, bool, Dict]:

        if action in self.actions:
            self.actions[action]()
            observation = self.getObs()  

            reward = self.calculate_reward() 

            done = self.check_done()  
            
            return observation, reward, done, {}
        else:
            raise ValueError(f"Unbekannte Aktion: {action}")
        
    def check_done(self):
        if self.health <= 0:
            return True
        return False

    def enemy_defeated(self):
        self.enemies_defeated += 1
        print(f"Feind besiegt! Anzahl besiegter Feinde: {self.enemies_defeated}")

    def calculate_reward(self) -> float:
        reward = 0

        reward += self.enemies_defeated  

        if self.health < 100:
            reward -= (100 - self.health)  

        return reward

    def perform_action(self, action: Actions):

        if action in self.actions:
            self.actions[action]()
        else:
            print(f"Unbekannte Aktion: {action}")

    def perform_action(self, action: Actions):
        if action in self.actions:
            self.actions[action]()
        else:
            print(f"Unbekannte Aktion: {action}")

    def walking_forward(self):
        print("Gehe vorwärts")
        self.keyboard.press('w')
        time.sleep(0.1)
        self.keyboard.release('w')

    def walking_backward(self):
        print("Gehe rückwärts")
        self.keyboard.press('s')
        time.sleep(0.1)
        self.keyboard.release('s')

    def aim(self):
        print("Zielen")
        self.keyboard.press(Key.left)  # Beispiel für das Drücken einer Maustaste
        time.sleep(0.1)
        self.keyboard.release(Key.left)

    def shoot(self):
        print("Schießen")
        self.enemy_defeated()
        self.keyboard.press(Key.right)  # Beispiel für das Drücken einer Maustaste
        time.sleep(0.1)
        self.keyboard.release(Key.right)

    def reload(self):
        print("Reload")
        self.aim()
        time.sleep(0.1)
        self.keyboard.press('r')
        time.sleep(0.1)
        self.keyboard.release('r')

    def turn_around_fast(self):
        print("TURN_AROUND_FAST")
        self.keyboard.press('s')
        time.sleep(0.1)
        self.keyboard.release('s')
        self.keyboard.press('s')
        time.sleep(0.1)
        self.keyboard.release('s')

    def getObs(self):
        # Screenshot vom Bildschirm machen
        screenshot = pyautogui.screenshot()
        img_array = np.array(screenshot)

        # Überprüfen, ob das Bild RGB istr
        if len(img_array.shape) == 3 and img_array.shape[-1] == 3:
            # Umwandlung von RGB in Graustufen
            img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        else:
            print(f"Unerwartete Anzahl an Kanälen: {img_array.shape[-1]}")

        # Bildgröße auf 84x84 ändern
        img_array = cv2.resize(img_array, (84, 84))

        # Normalisierung des Bildes
        img_array = img_array / 255.0  

        # Hinzufügen einer Dimension für das Batch (auf 1, 84, 84)
        img_array = np.expand_dims(img_array, axis=0)

        # Zum Anzeigen das Bild in der richtigen Form vorbereiten (von (1, 84, 84) nach (84, 84))
        img_display = img_array[0]  # Nimm das erste Bild aus dem Batch

        # Bild anzeigen (zum Debuggen)
        cv2.imshow('Screen Capture', img_display)  # Nur das Bild ohne zusätzliche Dimension anzeigen
        cv2.waitKey(1)  # Warte auf eine kurze Zeit, um das Bild anzuzeigen

        # Überprüfen der Form des Bildes
        print(img_array.shape)
    
        return img_array
    
    def __len__(self):
        return len(self.actions)
    
    def focus_game_window(self):
        window = gw.getWindowsWithTitle('Resident Evil')[0]  # Den genauen Fensternamen herausfinden
        window.activate()
        time.sleep(1)  # Warte, bis das Fenster den Fokus hat

if __name__ == "__main__":
    
    env = ResidentEvilEnv()

    agent = Agent(lr=1e-3,
                  input_dims= env.getObs().shape,
                  n_actions = env.__len__(),
                  max_buffer_size= 10000,
                  batch_size= 200,
                  replace= 1000,
                  eps= 1.0,
                  eps_dec = 1e-3,
                  gamma= 0.99,
                  model_path= r"I:\Sonstiges\Programmieren\Machine Learning\Part 6 - Reinforcement Learning",
                  chechpoint_path= r"I:\Sonstiges\Programmieren\Machine Learning\Part 6 - Reinforcement Learning")

    rewards = []
    game_iters = 1000
    scores = []
    env.focus_game_window()

    for i in range(game_iters):

        done = False

        truncated = False
        score = 0

        obs = env.getObs()

        while not done and not truncated :
            actions = agent.choose_actions(obs)
            env.perform_action(actions)
            observation_, reward, done, info = env.step(actions)
            agent.sample_obs_batches(observation_, obs, actions, reward, done)
            observations = obs
            agent.learn()
        if i % 100 == 0:
            agent.Main_Network.saving_model()
